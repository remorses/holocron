import { SearchClient } from './sdk.js'
import { importFromGitHub } from './import-github.js'

// Use a unique dataset ID for each test run with timestamp
const datasetId = `test-lancedb-benchmark`

// Initialize client (defaults to LanceDB cloud)
// Use environment variable if provided, otherwise use default cloud database
const lanceDbUri = process.env.LANCEDB_URI
const dataset = new SearchClient(lanceDbUri)

if (!lanceDbUri || lanceDbUri === 'db://fumabase-co7ad3') {
  console.log('‚òÅÔ∏è  Using LanceDB Cloud (fumabase-co7ad3)')
} else if (lanceDbUri.startsWith('db://')) {
  console.log('‚òÅÔ∏è  Using LanceDB Cloud:', lanceDbUri)
} else {
  console.log('üíæ Using local LanceDB at:', lanceDbUri)
}

// Setup: Create dataset and upload test files
console.log(`\nüìä Starting benchmark with dataset: ${datasetId}`)
console.log('üîß Creating dataset')
const upsertStartTime = Date.now()

// Create the dataset
await dataset.upsertDataset({ datasetId })

const upsertTime = (Date.now() - upsertStartTime) / 1000
console.log(`‚úÖ Dataset created in ${upsertTime.toFixed(2)} seconds`)

// Import from GitHub repository
console.log('üì• Importing from GitHub repository...')
const importStartTime = Date.now()

const importResult = await importFromGitHub({
  dataset,
  datasetId,
  owner: 'tldr-pages',
  repo: 'tldr',
  branch: 'main',
  path: 'pages', // Directory with markdown files
})

const importTime = (Date.now() - importStartTime) / 1000
console.log(`‚úÖ Import completed in ${importTime.toFixed(2)} seconds`)
console.log(`üìÅ Files imported: ${importResult.filesImported}`)
console.log(`üíæ Total size: ${(importResult.totalSizeBytes / 1024 / 1024).toFixed(2)} MB`)

if (importResult.filesImported === 0) {
  console.error('‚ùå No files were imported. Please check the repository and path.')
  process.exit(1)
}

// Get dataset statistics
const stats = await dataset.getDatasetSize({ datasetId })
console.log(`üìà Sections created: ${stats.sectionCount}`)

console.log(`‚è±Ô∏è  Total setup time: ${((Date.now() - upsertStartTime) / 1000).toFixed(2)} seconds`)

// Define search queries to benchmark
const searchQueries = [
  { query: 'file', description: 'File operations' },
  { query: 'git', description: 'Version control' },
  { query: 'docker', description: 'Containerization' },
  { query: 'python', description: 'Programming language' },
  { query: 'network', description: 'Networking' },
]

// Test search functionality before benchmarking
console.log('\nüîç Testing search functionality...')
try {
  const testResult = await dataset.searchSections({
    datasetId,
    query: searchQueries[0].query,
    perPage: 10,
  })
  console.log(`‚úÖ Search working! Found ${testResult.results.length} results for "${searchQueries[0].query}"`)
  if (testResult.results.length > 0) {
    console.log(
      `   First result: ${testResult.results[0].filename} - ${testResult.results[0].snippet.substring(0, 50)}...`,
    )
  }
} catch (error) {
  console.error('‚ùå Search test failed:', error)
  process.exit(1)
}

// Log search result counts
console.log('\nüìä Search result counts:')
for (const { query, description } of searchQueries) {
  try {
    const result = await dataset.searchSections({
      datasetId,
      query,
      perPage: 20,
    })
    console.log(`  "${query}" (${description}): ${result.results.length} results`)
  } catch (error: any) {
    console.error(`  "${query}": ERROR - ${error.message}`)
  }
}

// Run benchmarks
const ITERATIONS = 5
console.log(`\nüèÉ Running benchmarks (${ITERATIONS} iterations per query)...\n`)
console.log('LanceDB Search Benchmark')
console.log('========================\n')

const results: Array<{ query: string; times: number[]; avg: number }> = []

// Double loop: queries and iterations
for (const { query, description } of searchQueries) {
  console.log(`Benchmarking "${query}" (${description}):`)
  const times: number[] = []

  for (let i = 0; i < ITERATIONS; i++) {
    const startTime = performance.now()
    await dataset.searchSections({
      datasetId,
      query,
      perPage: 10,
    })
    const elapsed = performance.now() - startTime
    times.push(elapsed)
    console.log(`  Run ${i + 1}: ${elapsed.toFixed(2)}ms`)
  }

  const avg = times.reduce((sum, t) => sum + t, 0) / times.length
  results.push({ query, times, avg })
  console.log(`  Average: ${avg.toFixed(2)}ms\n`)
}

// Display summary results
console.log('\nüìä Summary Results:\n')
console.log('Query             | Avg (ms) | Min (ms) | Max (ms)')
console.log('------------------|----------|----------|----------')
for (const result of results) {
  const paddedQuery = result.query.padEnd(17)
  const min = Math.min(...result.times)
  const max = Math.max(...result.times)
  console.log(
    `${paddedQuery} | ${result.avg.toFixed(2).padStart(8)} | ${min.toFixed(2).padStart(8)} | ${max.toFixed(2).padStart(8)}`,
  )
}

// Calculate and show overall average
const overallAvg = results.reduce((sum, r) => sum + r.avg, 0) / results.length
console.log('------------------|----------|----------|----------')
console.log(`Overall Average   | ${overallAvg.toFixed(2).padStart(8)} |          |`)

// Cleanup
console.log('\nüßπ Cleaning up...')
try {
  await dataset.deleteDataset({ datasetId })
  console.log('‚úÖ Dataset deleted successfully')
} catch (error) {
  console.warn('‚ö†Ô∏è  Failed to cleanup dataset:', error)
}

console.log('\n‚úÖ Benchmark complete!')
console.log(`Dataset ID: ${datasetId}`)
console.log(`Repository: tldr-pages/tldr`)
console.log(`Path: pages/`)
console.log(`Iterations per query: ${ITERATIONS}`)
